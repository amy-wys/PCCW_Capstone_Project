{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a22b270",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain==0.0.142 openai==0.27.4 beautifulsoup4==4.12.2 chromadb==0.3.21 GitPython==3.1.31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8513347e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "from langchain.document_loaders import (\n",
    "    GitLoader,\n",
    "    DataFrameLoader\n",
    ")\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings import OpenAIEmbeddings, HuggingFaceInstructEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.llms import OpenAI, HuggingFaceHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "HUGGINGFACEHUB_API_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfeb22bf",
   "metadata": {},
   "source": [
    "# Implementing a sales & support agent with LangChain\n",
    "## Learn how to develop a chatbot that can answer questions based on the information provided in your company's documentation\n",
    "\n",
    "Recently, I have been fascinated by the power of ChatGPT and its ability to construct various types of chatbots. I have tried and written about multiple approaches to implementing a chatbot that can access external information to improve its answers. I joined a few Discord channels during my chatbot coding sessions, hoping to get some help as the libraries are relatively new, and not much documentation is available yet. To my amazement, I found custom bots that could answer most of the questions for the given library.\n",
    "\n",
    "The idea is to provide the chatbot the ability to dig through various resources like company documentation, code, or other content in order to allow it to answer company support questions. Since I already have some experience with chatbots, I decided to test how hard it is to implement a custom bot with access to the company's resources.\n",
    "In this blog post, I will walk you through how I used OpenAI's models to implement a sales & support agent with in the LangChain library that can be used to answer information about applications with a graph database Neo4j. The agent can also help you debug or produce any Cypher statement you are struggling with. Such an agent could then be deployed to serve users on Discord or other platforms.\n",
    "\n",
    "We will be using the LangChain library to implement the support bot. The library is easy to use and provides an excellent integration of LLM prompts and Python code, allowing us to develop chatbots in only a few minutes. In addition, the library supports a range of LLMs, text embedding models, and vector databases, along with utility functions that help us load and embed frequent types of files we might come across, like text, PowerPoint, images, HTML, PDF, and more.\n",
    "\n",
    "## LangChain document loaders\n",
    "First, we must preprocess the company's resources and store them in a vector database. Luckily, LangChain can help us load external data, calculate text embeddings, and store the documents in a vector database of our choice.\n",
    "First, we have to load the text into documents. LangChain offers a variety of helper functions that can take various formats and types of data and produce a document output. The helper functions are called Document loaders.\n",
    "\n",
    "Neo4j has a lot of its documentation available in GitHub repositories. Conveniently, LangChain provides a document loader that takes a repository URL as input and produces a document for each file in the repository. Additionally, we can use the filter function to ignore files during the loading process if needed.\n",
    "\n",
    "We will begin by loading the AsciiDoc files from the Neo4j's knowledge base repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee481063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "309\n"
     ]
    }
   ],
   "source": [
    "# Knowledge base\n",
    "kb_loader = GitLoader(\n",
    "    clone_url=\"https://github.com/neo4j-documentation/knowledge-base\",\n",
    "    repo_path=\"./repos/kb/\",\n",
    "    branch=\"master\",\n",
    "    file_filter=lambda file_path: file_path.endswith(\".adoc\")\n",
    "    and \"articles\" in file_path,\n",
    ")\n",
    "kb_data = kb_loader.load()\n",
    "print(len(kb_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5383390",
   "metadata": {},
   "source": [
    "Wasn't that easy as a pie? The GitLoader function clones the repository and load relevant files as documents. In this example, we specified that the file must end with .adoc suffix and be a part of the articles folder. In total, 309 articles were loaded. We also have to be mindful of the size of the documents. For example, GPT-3.5-turbo has a token limit of 4000, while GPT-4 allows 8000 tokens in a single request. While number of words is not exactly identical to the number of tokens, it is still a good estimator. \n",
    "\n",
    "Next, we will load the documentation of the Graph Data Science repository. Here, we will use a text splitter to make sure none of the documents exceed 2000 words. Again, I know that number of words is not equal to the number of tokens, but it is a good approximation. Defining the threshold number of tokens can significantly affect how the database is found and retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f45bdb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "819\n"
     ]
    }
   ],
   "source": [
    "# Define text chunk strategy\n",
    "splitter = CharacterTextSplitter(\n",
    "  chunk_size=2000, \n",
    "  chunk_overlap=50,\n",
    "  separator=\" \"\n",
    ")\n",
    "# GDS guides\n",
    "gds_loader = GitLoader(\n",
    "    clone_url=\"https://github.com/neo4j/graph-data-science\",\n",
    "    repo_path=\"./repos/gds/\",\n",
    "    branch=\"master\",\n",
    "    file_filter=lambda file_path: file_path.endswith(\".adoc\") \n",
    "    and \"pages\" in file_path,\n",
    ")\n",
    "gds_data = gds_loader.load()\n",
    "# Split documents into chunks\n",
    "gds_data_split = splitter.split_documents(gds_data)\n",
    "print(len(gds_data_split))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70afdb4b",
   "metadata": {},
   "source": [
    "We could load other Neo4j repositories that contain documentation. However, the idea is to show various data loading methods and not explore all of Neo4j's repositories containing documentation. Therefore, we will move on and look at how we can load documents from a Pandas Dataframe.\n",
    "\n",
    "For example, say that we want to load a YouTube video as a document source for our chatbot. Neo4j has its own YouTube channel and, even I appear in a video or two. Two years ago I presented how to implement an information extraction pipeline.\n",
    "With LangChain, we can use the captions of the video and load it as documents with only three lines of code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9220b7",
   "metadata": {},
   "source": [
    "It couldn't get any easier than this. Next, we will look at loading documents from a Pandas dataframe. A month ago, I retrieved information from Neo4j medium publication for a separate blog post. Since we want to bring external information about Neo4j to the bot, we can also use the content of the medium articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c81c834c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2209\n"
     ]
    }
   ],
   "source": [
    "# Medium\n",
    "article_url = \"https://raw.githubusercontent.com/tomasonjo/blog-datasets/main/medium/neo4j_articles.csv\"\n",
    "medium = pd.read_csv(article_url, sep=\";\")\n",
    "medium[\"source\"] = medium[\"url\"]\n",
    "medium_loader = DataFrameLoader(\n",
    "    medium[[\"text\", \"source\"]], \n",
    "    page_content_column=\"text\")\n",
    "medium_data = medium_loader.load()\n",
    "medium_data_split = splitter.split_documents(medium_data)\n",
    "print(len(medium_data_split))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0ac744",
   "metadata": {},
   "source": [
    "Here, we used Pandas to load a CSV file from GitHub, renamed one column, and used the DataFrameLoaderfunction to load the articles as documents. Since medium posts could exceed 4000 tokens, we used the text splitter to split the articles into multiple chunks.\n",
    "\n",
    "The last source we will use is the Stack Overflow API. Stack Overflow is a web platform where users help others solve coding problems. Their API does not require any authorization. Therefore, we can use the API to retrieve questions with accepted answers that are tagged with the Neo4j tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1a0e03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "797\n"
     ]
    }
   ],
   "source": [
    "# Stackoverflow\n",
    "so_data = []\n",
    "for i in range(1, 20):\n",
    "    # Define the Stack Overflow API endpoint and parameters\n",
    "    api_url = \"https://api.stackexchange.com/2.3/questions\"\n",
    "    params = {\n",
    "        \"order\": \"desc\",\n",
    "        \"sort\": \"creation\",\n",
    "        \"filter\": \"!-MBrU_IzpJ5H-AG6Bbzy.X-BYQe(2v-.J\",\n",
    "        \"tagged\": \"neo4j\",\n",
    "        \"site\": \"stackoverflow\",\n",
    "        \"pagesize\": 100,\n",
    "        \"page\": i,\n",
    "    }\n",
    "    # Send GET request to Stack Overflow API\n",
    "    response = requests.get(api_url, params=params)\n",
    "    data = response.json()\n",
    "    # Retrieve the resolved questions\n",
    "    resolved_questions = [\n",
    "        question\n",
    "        for question in data[\"items\"]\n",
    "        if question[\"is_answered\"] and question.get(\"accepted_answer_id\")\n",
    "    ]\n",
    "\n",
    "    # Print the resolved questions\n",
    "    for question in resolved_questions:\n",
    "        text = (\n",
    "            \"Title:\",\n",
    "            question[\"title\"] + \"\\n\" + \"Question:\",\n",
    "            BeautifulSoup(question[\"body\"]).get_text()\n",
    "            + \"\\n\"\n",
    "            + BeautifulSoup(\n",
    "                [x[\"body\"] for x in question[\"answers\"] if x[\"is_accepted\"]][0]\n",
    "            ).get_text(),\n",
    "        )\n",
    "        source = question[\"link\"]\n",
    "        so_data.append(Document(page_content=str(text), metadata={\"source\": source}))\n",
    "print(len(so_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51352c2",
   "metadata": {},
   "source": [
    "Each approved answer and the original question are used to construct a single document. Since most Stack overflow questions and answers do not exceed 4000 tokens, we skipped the text-splitting step.\n",
    "Now that we have loaded the documentation resources as documents, we can move on to the next step.\n",
    "# Storing documents in a vector database\n",
    "A chatbot finds relevant information by comparing the vector embedding of questions with document embeddings. A text embedding is a machine-readable representation of text in the form of a vector or, more plainly, a list of floats. In this example, we will use the ada-002 model provided by OpenAI to embed documents.\n",
    "The whole idea behind vector databases is the ability to store vectors and provide fast similarity searches. The vectors are usually compared using cosine similarity. LangChain includes integration with a variety of vector databases. To keep things simple, we will use the Chroma vector database, which can be used as a local in-memory. For a more serious chatbot application, we want to use a persistent database that doesn't lose data once the script or notebook is closed.\n",
    "\n",
    "We will create two collections of documents. The first will be more sales and marketing oriented, containing documents from Medium and YouTube. The second collection focuses more on support use cases and consists of documentation and Stack Overflow documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8955bd54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hytung/Library/Python/3.9/lib/python/site-packages/InstructorEmbedding/instructor.py:7: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/hytung/Documents/GitHub/cef/PCCW_Capstone_Project/neo4j_hug.ipynb Cell 14\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hytung/Documents/GitHub/cef/PCCW_Capstone_Project/neo4j_hug.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m embeddings \u001b[39m=\u001b[39m HuggingFaceInstructEmbeddings(model_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBAAI/bge-large-en-v1.5\u001b[39m\u001b[39m\"\u001b[39m, model_kwargs\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m})\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hytung/Documents/GitHub/cef/PCCW_Capstone_Project/neo4j_hug.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m sales_data \u001b[39m=\u001b[39m medium_data_split \n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/hytung/Documents/GitHub/cef/PCCW_Capstone_Project/neo4j_hug.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m sales_store \u001b[39m=\u001b[39m Chroma\u001b[39m.\u001b[39;49mfrom_documents(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hytung/Documents/GitHub/cef/PCCW_Capstone_Project/neo4j_hug.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     sales_data, embeddings, collection_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msales\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hytung/Documents/GitHub/cef/PCCW_Capstone_Project/neo4j_hug.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m )\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hytung/Documents/GitHub/cef/PCCW_Capstone_Project/neo4j_hug.ipynb#X16sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m support_data \u001b[39m=\u001b[39m kb_data \u001b[39m+\u001b[39m gds_data_split \u001b[39m+\u001b[39m so_data\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hytung/Documents/GitHub/cef/PCCW_Capstone_Project/neo4j_hug.ipynb#X16sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m support_store \u001b[39m=\u001b[39m Chroma\u001b[39m.\u001b[39mfrom_documents(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hytung/Documents/GitHub/cef/PCCW_Capstone_Project/neo4j_hug.ipynb#X16sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     support_data, embeddings, collection_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msupport\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hytung/Documents/GitHub/cef/PCCW_Capstone_Project/neo4j_hug.ipynb#X16sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain/vectorstores/chroma.py:771\u001b[0m, in \u001b[0;36mChroma.from_documents\u001b[0;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    769\u001b[0m texts \u001b[39m=\u001b[39m [doc\u001b[39m.\u001b[39mpage_content \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m documents]\n\u001b[1;32m    770\u001b[0m metadatas \u001b[39m=\u001b[39m [doc\u001b[39m.\u001b[39mmetadata \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m documents]\n\u001b[0;32m--> 771\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mfrom_texts(\n\u001b[1;32m    772\u001b[0m     texts\u001b[39m=\u001b[39;49mtexts,\n\u001b[1;32m    773\u001b[0m     embedding\u001b[39m=\u001b[39;49membedding,\n\u001b[1;32m    774\u001b[0m     metadatas\u001b[39m=\u001b[39;49mmetadatas,\n\u001b[1;32m    775\u001b[0m     ids\u001b[39m=\u001b[39;49mids,\n\u001b[1;32m    776\u001b[0m     collection_name\u001b[39m=\u001b[39;49mcollection_name,\n\u001b[1;32m    777\u001b[0m     persist_directory\u001b[39m=\u001b[39;49mpersist_directory,\n\u001b[1;32m    778\u001b[0m     client_settings\u001b[39m=\u001b[39;49mclient_settings,\n\u001b[1;32m    779\u001b[0m     client\u001b[39m=\u001b[39;49mclient,\n\u001b[1;32m    780\u001b[0m     collection_metadata\u001b[39m=\u001b[39;49mcollection_metadata,\n\u001b[1;32m    781\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    782\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain/vectorstores/chroma.py:729\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mchromadb\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbatch_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m create_batches\n\u001b[1;32m    723\u001b[0m     \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m create_batches(\n\u001b[1;32m    724\u001b[0m         api\u001b[39m=\u001b[39mchroma_collection\u001b[39m.\u001b[39m_client,\n\u001b[1;32m    725\u001b[0m         ids\u001b[39m=\u001b[39mids,\n\u001b[1;32m    726\u001b[0m         metadatas\u001b[39m=\u001b[39mmetadatas,\n\u001b[1;32m    727\u001b[0m         documents\u001b[39m=\u001b[39mtexts,\n\u001b[1;32m    728\u001b[0m     ):\n\u001b[0;32m--> 729\u001b[0m         chroma_collection\u001b[39m.\u001b[39;49madd_texts(\n\u001b[1;32m    730\u001b[0m             texts\u001b[39m=\u001b[39;49mbatch[\u001b[39m3\u001b[39;49m] \u001b[39mif\u001b[39;49;00m batch[\u001b[39m3\u001b[39;49m] \u001b[39melse\u001b[39;49;00m [],\n\u001b[1;32m    731\u001b[0m             metadatas\u001b[39m=\u001b[39;49mbatch[\u001b[39m2\u001b[39;49m] \u001b[39mif\u001b[39;49;00m batch[\u001b[39m2\u001b[39;49m] \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    732\u001b[0m             ids\u001b[39m=\u001b[39;49mbatch[\u001b[39m0\u001b[39;49m],\n\u001b[1;32m    733\u001b[0m         )\n\u001b[1;32m    734\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    735\u001b[0m     chroma_collection\u001b[39m.\u001b[39madd_texts(texts\u001b[39m=\u001b[39mtexts, metadatas\u001b[39m=\u001b[39mmetadatas, ids\u001b[39m=\u001b[39mids)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain/vectorstores/chroma.py:275\u001b[0m, in \u001b[0;36mChroma.add_texts\u001b[0;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m texts \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(texts)\n\u001b[1;32m    274\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_embedding_function \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 275\u001b[0m     embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_embedding_function\u001b[39m.\u001b[39;49membed_documents(texts)\n\u001b[1;32m    276\u001b[0m \u001b[39mif\u001b[39;00m metadatas:\n\u001b[1;32m    277\u001b[0m     \u001b[39m# fill metadatas with empty dicts if somebody\u001b[39;00m\n\u001b[1;32m    278\u001b[0m     \u001b[39m# did not specify metadata for all texts\u001b[39;00m\n\u001b[1;32m    279\u001b[0m     length_diff \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(texts) \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(metadatas)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain/embeddings/huggingface.py:171\u001b[0m, in \u001b[0;36mHuggingFaceInstructEmbeddings.embed_documents\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Compute doc embeddings using a HuggingFace instruct model.\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \n\u001b[1;32m    164\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[39m    List of embeddings, one for each text.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    170\u001b[0m instruction_pairs \u001b[39m=\u001b[39m [[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_instruction, text] \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m texts]\n\u001b[0;32m--> 171\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mencode(instruction_pairs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode_kwargs)\n\u001b[1;32m    172\u001b[0m \u001b[39mreturn\u001b[39;00m embeddings\u001b[39m.\u001b[39mtolist()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/InstructorEmbedding/instructor.py:539\u001b[0m, in \u001b[0;36mINSTRUCTOR.encode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    536\u001b[0m features \u001b[39m=\u001b[39m batch_to_device(features, device)\n\u001b[1;32m    538\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 539\u001b[0m     out_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(features)\n\u001b[1;32m    541\u001b[0m     \u001b[39mif\u001b[39;00m output_value \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtoken_embeddings\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    542\u001b[0m         embeddings \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/InstructorEmbedding/instructor.py:269\u001b[0m, in \u001b[0;36mINSTRUCTOR_Transformer.forward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcontext_masks\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m features:\n\u001b[1;32m    268\u001b[0m     context_masks \u001b[39m=\u001b[39m features[\u001b[39m'\u001b[39m\u001b[39mcontext_masks\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m--> 269\u001b[0m output_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mauto_model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtrans_features, return_dict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    270\u001b[0m output_tokens \u001b[39m=\u001b[39m output_states[\u001b[39m0\u001b[39m]\n\u001b[1;32m    271\u001b[0m attention_mask \u001b[39m=\u001b[39m features[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/bert/modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1004\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1006\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m   1007\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1008\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1012\u001b[0m )\n\u001b[0;32m-> 1013\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1014\u001b[0m     embedding_output,\n\u001b[1;32m   1015\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   1016\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1017\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1018\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   1019\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1020\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1021\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1022\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1023\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1024\u001b[0m )\n\u001b[1;32m   1025\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1026\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/bert/modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    596\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    597\u001b[0m         layer_module\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m    598\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    604\u001b[0m         output_attentions,\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    606\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 607\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    608\u001b[0m         hidden_states,\n\u001b[1;32m    609\u001b[0m         attention_mask,\n\u001b[1;32m    610\u001b[0m         layer_head_mask,\n\u001b[1;32m    611\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    612\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    613\u001b[0m         past_key_value,\n\u001b[1;32m    614\u001b[0m         output_attentions,\n\u001b[1;32m    615\u001b[0m     )\n\u001b[1;32m    617\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    618\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/bert/modeling_bert.py:539\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    536\u001b[0m     cross_attn_present_key_value \u001b[39m=\u001b[39m cross_attention_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    537\u001b[0m     present_key_value \u001b[39m=\u001b[39m present_key_value \u001b[39m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 539\u001b[0m layer_output \u001b[39m=\u001b[39m apply_chunking_to_forward(\n\u001b[1;32m    540\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeed_forward_chunk, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk_size_feed_forward, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseq_len_dim, attention_output\n\u001b[1;32m    541\u001b[0m )\n\u001b[1;32m    542\u001b[0m outputs \u001b[39m=\u001b[39m (layer_output,) \u001b[39m+\u001b[39m outputs\n\u001b[1;32m    544\u001b[0m \u001b[39m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/pytorch_utils.py:241\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[39m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    239\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(output_chunks, dim\u001b[39m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 241\u001b[0m \u001b[39mreturn\u001b[39;00m forward_fn(\u001b[39m*\u001b[39;49minput_tensors)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/bert/modeling_bert.py:552\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfeed_forward_chunk\u001b[39m(\u001b[39mself\u001b[39m, attention_output):\n\u001b[1;32m    551\u001b[0m     intermediate_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> 552\u001b[0m     layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput(intermediate_output, attention_output)\n\u001b[1;32m    553\u001b[0m     \u001b[39mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/bert/modeling_bert.py:464\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: torch\u001b[39m.\u001b[39mTensor, input_tensor: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 464\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdense(hidden_states)\n\u001b[1;32m    465\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    466\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayerNorm(hidden_states \u001b[39m+\u001b[39m input_tensor)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define embedding model\n",
    "embeddings = HuggingFaceInstructEmbeddings(model_name=\"BAAI/bge-large-en-v1.5\", model_kwargs={'device': 'cpu'})\n",
    "\n",
    "sales_data = medium_data_split \n",
    "sales_store = Chroma.from_documents(\n",
    "    sales_data, embeddings, collection_name=\"sales\"\n",
    ")\n",
    "\n",
    "support_data = kb_data + gds_data_split + so_data\n",
    "support_store = Chroma.from_documents(\n",
    "    support_data, embeddings, collection_name=\"support\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7176f1bf",
   "metadata": {},
   "source": [
    "This script runs each document through OpenAI's text embedding API and inserts the resulting embedding along with text in the Chroma database. The process of text embedding costs 0.80$, which is a reasonable price.\n",
    "## Question answering using external context\n",
    "\n",
    "The last thing to do is to implement two separate question-answering flow. The first will handle the sales & marketing requests, while the other will handle support. The LangChain library uses LLMs for reasoning and providing answers to the user. Therefore, we start by defining the LLM. Here, we will be using the GPT-3.5-turbo model from OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ac3cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFaceHub(\n",
    "        repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "        model_kwargs={\"temperature\": 0.2, \"max_length\": 256},\n",
    "        huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3c7e9a",
   "metadata": {},
   "source": [
    "Implementing a question-answering flow is about as easy as it gets with LangChain. We only need to provide the LLM to be used along with the retriever that is used to fetch relevant documents. Additionally, we have the option to customize the LLM prompt used to answer questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8b1bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_template = \"\"\"As a Neo4j marketing bot, your goal is to provide accurate and helpful information about Neo4j,\n",
    "a powerful graph database used for building various applications.\n",
    "You should answer user inquiries based on the context provided and avoid making up answers.\n",
    "If you don't know the answer, simply state that you don't know.\n",
    "Remember to provide relevant information about Neo4j's features, benefits,\n",
    "and use cases to assist the user in understanding its value for application development.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\"\"\"\n",
    "SALES_PROMPT = PromptTemplate(\n",
    "    template=sales_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "sales_qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=sales_store.as_retriever(),\n",
    "    chain_type_kwargs={\"prompt\": SALES_PROMPT},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08b7e9c",
   "metadata": {},
   "source": [
    "The most important part of the sales prompt is to prohibit the LLM from basing its responses without relying on official company resources. Remember, LLMs can act very assertively while providing invalid information. However, we would like to avoid that scenario and avoid getting into problems where the bot promised or sold non-existing features. We can test the sales question answering flow by asking the following question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c38f909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main benefits of using Neo4j are:\n",
      "\n",
      "1. Graph-based Data Model: Neo4j uses a graph-based data model, which allows for flexible and efficient representation of complex relationships between data entities. This makes it easier to model and query connected data, enabling more accurate and insightful analysis.\n",
      "\n",
      "2. High Performance: Neo4j is designed for high performance, allowing for fast and efficient querying of large datasets. Its native graph storage and traversal capabilities enable quick retrieval of connected data, making it ideal for applications that require real-time insights.\n",
      "\n",
      "3. Scalability: Neo4j is highly scalable and can handle billions of nodes and relationships. It can be easily scaled horizontally across multiple machines to accommodate growing data volumes and user demands.\n",
      "\n",
      "4. Powerful Query Language: Neo4j uses the Cypher query language, which is specifically designed for querying graph data. Cypher is expressive and intuitive, allowing developers to easily write complex queries to retrieve and manipulate data.\n",
      "\n",
      "5. Flexibility and Agility: Neo4j's schema-less nature allows for easy and agile development. It does not require a predefined schema, making it suitable for applications with evolving data structures. This flexibility enables faster development cycles and easier adaptation to changing business requirements.\n",
      "\n",
      "6. Use Case Versatility: Neo4j is applicable to a wide range of use cases, including social networks, recommendation engines, fraud detection, knowledge graphs, and more. Its graph-based data model and powerful query capabilities make it a versatile tool for various application domains.\n",
      "\n",
      "Overall, Neo4j offers a powerful and efficient solution for managing and analyzing connected data, providing developers with the tools they need to build robust and scalable applications.\n"
     ]
    }
   ],
   "source": [
    "print(sales_qa.run(\"What are the main benefits of using Neo4j?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec9aa44",
   "metadata": {},
   "source": [
    "The response to the question seems relevant and accurate. Remember, the information to construct this response came from Medium articles.\n",
    "\n",
    "Next, we will implement the support question-answering flow. Here, we will allow the LLM model to use its knowledge of Cypher and Neo4j to help solve the user's problem if the context doesn't provide enough information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10cdf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "support_template = \"\"\"\n",
    "As a Neo4j Customer Support bot, you are here to assist with any issues \n",
    "a user might be facing with their graph database implementation and Cypher statements.\n",
    "Please provide as much detail as possible about the problem, how to solve it, and steps a user should take to fix it.\n",
    "If the provided context doesn't provide enough information, you are allowed to use your knowledge and experience to offer you the best possible assistance.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\"\"\"\n",
    "\n",
    "SUPPORT_PROMPT = PromptTemplate(\n",
    "    template=support_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "support_qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=support_store.as_retriever(),\n",
    "    chain_type_kwargs={\"prompt\": SUPPORT_PROMPT},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b84a31",
   "metadata": {},
   "source": [
    "And again, we can test the support question-answering abilities. I took a random question from Neo4j's discord server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bfcb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To install the GDS plugin in your VM, you can follow these steps:\n",
      "\n",
      "1. Download the compatible version of the Neo4j Graph Data Science Library from the Neo4j website. Make sure the version matches the version of Neo4j running on your VM.\n",
      "\n",
      "2. Once downloaded, decompress the file.\n",
      "\n",
      "3. Locate the `neo4j-graph-data-science-[version].jar` file from the decompressed folder.\n",
      "\n",
      "4. Move the `neo4j-graph-data-science-[version].jar` file into the `plugins` directory of your Neo4j installation on the VM. The location of the `plugins` directory depends on the type of installation:\n",
      "\n",
      "   - For a standalone Neo4j Server, the `plugins` directory is typically located at `$NEO4J_HOME/plugins`. You can check the value of `$NEO4J_HOME` by executing the command `sudo systemctl status neo4j` and looking at the \"Loaded: ...\" line in the result.\n",
      "\n",
      "   - For Neo4j Desktop, you can find the `plugins` directory in the database folder of your Neo4j project.\n",
      "\n",
      "5. Open the `$NEO4J_HOME/conf/neo4j.conf` file (or the configuration file for your Neo4j installation) and add the following configuration entry to allow unrestricted access to the GDS procedures:\n",
      "\n",
      "   ```\n",
      "   dbms.security.procedures.unrestricted=gds.*\n",
      "   ```\n",
      "\n",
      "   This configuration entry is necessary because the GDS library accesses low-level components of Neo4j to maximize performance.\n",
      "\n",
      "6. If the procedure allowlist is enabled in the configuration file, make sure to include the GDS library in the allowlist as well:\n",
      "\n",
      "   ```\n",
      "   dbms.security.procedures.allowlist=gds.*\n",
      "   ```\n",
      "\n",
      "   This step ensures that the GDS procedures are allowed to be executed.\n",
      "\n",
      "7. Restart your Neo4j server or Neo4j Desktop to apply the changes.\n",
      "\n",
      "After following these steps, the GDS plugin should be installed and available for use in your VM. You can then use the GDS procedures in your Cypher statements to perform graph data science operations.\n"
     ]
    }
   ],
   "source": [
    "print(support_qa.run(\"\"\"\n",
    "I am having my graph in a VM and i want to use GDS Plugins in my graph.\n",
    "I didn't see any proper documentation to install it in my server where it was working only on locally.\n",
    "Anyone clear me this to install the GDS Plugin in the VM ?\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6402502d",
   "metadata": {},
   "source": [
    "The response is quite to the point. Remember, we retrieved the Graph Data Science documentation and are using it as context to form the chatbot questions.\n",
    "## Agent implementation\n",
    "We now have two separate instructions and stores for sales and support responses. If we had to put a human in the loop to distinguish between the two, the whole point of the chatbot would be lost. Luckily, we can use a LangChain agent to decide which tool to use based on the user input. First, we need to define the available tools of an agent along with instructions on when and how to use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1179d1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the zero-shot-react-description agent will use the \"description\" string to select tools\n",
    "tools = [\n",
    "    Tool(\n",
    "        name = \"sales\",\n",
    "        func=sales_qa.run,\n",
    "        description=\"\"\"useful for when a user is interested in various Neo4j information, \n",
    "                       use-cases, or applications. A user is not asking for any debugging, but is only\n",
    "                       interested in general advice for integrating and using Neo4j.\n",
    "                       Input should be a fully formed question.\"\"\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name = \"support\",\n",
    "        func=support_qa.run,\n",
    "        description=\"\"\"useful for when when a user asks to optimize or debug a Cypher statement or needs\n",
    "                       specific instructions how to accomplish a specified task. \n",
    "                       Input should be a fully formed question.\"\"\"\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff7694f",
   "metadata": {},
   "source": [
    "The description of a tool is used by an agent to identify when and how to use a tool. For example, the support tool should be used to optimize or debug a Cypher statement and the input to the tool should be a fully formed question.\n",
    "\n",
    "The last thing we need to do is to initialize the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b335506",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec74be42",
   "metadata": {},
   "source": [
    "Now we can go ahead and test the agent on a couple of questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52863aac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should check if there are any specific GPT-4 applications with Neo4j mentioned in the sales tool.\n",
      "Action: sales\n",
      "Action Input: GPT-4 applications with Neo4j\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mGPT-4, as a powerful language model, can have various applications with Neo4j, a graph database. Here are a few potential applications:\n",
      "\n",
      "1. Knowledge Graph Construction: GPT-4 can be used to extract relevant entities and relationships from text, which can then be used to construct a knowledge graph in Neo4j. This can help in organizing and visualizing complex information.\n",
      "\n",
      "2. Natural Language Processing (NLP): GPT-4 can assist in NLP tasks such as sentiment analysis, named entity recognition, and text classification. By integrating GPT-4 with Neo4j, you can enhance the capabilities of your NLP applications by leveraging the graph database's querying and analysis capabilities.\n",
      "\n",
      "3. Chatbot Development: GPT-4 can be used to power chatbots that interact with users in a conversational manner. By integrating the chatbot with Neo4j, you can leverage the graph database to store and retrieve relevant information, providing more personalized and context-aware responses.\n",
      "\n",
      "4. Data Generation and Import: GPT-4 can assist in generating sample datasets for testing and development purposes. You can use GPT-4 to create synthetic data that follows a specific data model and then import it into Neo4j for further analysis and application development.\n",
      "\n",
      "5. Query Optimization: GPT-4 can be used to analyze and optimize queries in Neo4j. By providing query examples and performance requirements, GPT-4 can suggest query optimizations and indexing strategies to improve the efficiency of your graph database queries.\n",
      "\n",
      "These are just a few examples of how GPT-4 can be applied in conjunction with Neo4j. The combination of a powerful language model like GPT-4 and the graph database capabilities of Neo4j opens up a wide range of possibilities for application development and data analysis.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI have provided a comprehensive list of potential GPT-4 applications with Neo4j.\n",
      "Final Answer: Some potential GPT-4 applications with Neo4j include knowledge graph construction, natural language processing, chatbot development, data generation and import, and query optimization.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Some potential GPT-4 applications with Neo4j include knowledge graph construction, natural language processing, chatbot development, data generation and import, and query optimization.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"\"\"What are some GPT-4 applications with Neo4j?\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece1136f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThis sounds like a technical question that requires specific instructions or optimization. \n",
      "\n",
      "Action: support\n",
      "Action Input: Ask the user for more information about their specific use case and what they are trying to accomplish with the weighted shortest path query. Ask if they have tried any alternative methods or libraries. \u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAs a Neo4j Customer Support bot, I would like to ask for more information about your specific use case and what you are trying to accomplish with the weighted shortest path query. Have you tried any alternative methods or libraries? This will help me provide you with the best possible assistance.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mBased on the user's response, I may need to provide specific instructions or suggest alternative methods.\n",
      "\n",
      "Action: support\n",
      "Action Input: Based on the user's response, provide specific instructions or suggest alternative methods for executing a weighted shortest path query in the Neo4j Community edition.\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mTo execute a weighted shortest path query in the Neo4j Community edition, you can use the APOC library. Here's an example query:\n",
      "\n",
      "```\n",
      "MATCH (start:Node {name: \"Start\"}), (end:Node {name: \"End\"})\n",
      "CALL apoc.algo.dijkstraWithDefaultWeight(start, end, \"CONNECTED_TO\", \"weight\")\n",
      "YIELD path, weight\n",
      "RETURN path, weight\n",
      "```\n",
      "\n",
      "In this query, \"CONNECTED_TO\" is the name of the relationship type, and \"weight\" is the name of the property on the relationship that contains the weight. You can replace these values with the appropriate names for your graph.\n",
      "\n",
      "If you don't want to use the APOC library, you can also implement Dijkstra's algorithm manually in Cypher. Here's an example query:\n",
      "\n",
      "```\n",
      "MATCH (start:Node {name: \"Start\"}), (end:Node {name: \"End\"})\n",
      "WITH start, end\n",
      "MATCH path = (start)-[:CONNECTED_TO*]->(end)\n",
      "WHERE all(rel in relationships(path) WHERE rel.weight IS NOT NULL)\n",
      "WITH path, reduce(weight = 0, rel in relationships(path) | weight + rel.weight) AS totalWeight\n",
      "RETURN path, totalWeight\n",
      "ORDER BY totalWeight ASC\n",
      "LIMIT 1\n",
      "```\n",
      "\n",
      "In this query, \"CONNECTED_TO\" is the name of the relationship type, and \"weight\" is the name of the property on the relationship that contains the weight. You can replace these values with the appropriate names for your graph. This query finds the shortest path between the start and end nodes based on the sum of the weights of the relationships in the path.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer.\n",
      "\n",
      "Final Answer: To execute a weighted shortest path query in the Neo4j Community edition, you can use the APOC library or implement Dijkstra's algorithm manually in Cypher. The APOC library provides a built-in function for Dijkstra's algorithm, while the manual implementation requires a Cypher query that finds the shortest path based on the sum of the weights of the relationships in the path.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"To execute a weighted shortest path query in the Neo4j Community edition, you can use the APOC library or implement Dijkstra's algorithm manually in Cypher. The APOC library provides a built-in function for Dijkstra's algorithm, while the manual implementation requires a Cypher query that finds the shortest path based on the sum of the weights of the relationships in the path.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"\"\"\n",
    "Hello everyone, is there a way to execute a weighted shortest path query in the Neo4j Community edition?\n",
    "All I have found on the internet was gds library or the algo library, which are both unavailable the community version.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627daef9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
